{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6a771-9e99-4deb-ae5d-04bc3a9c020a",
   "metadata": {
    "id": "9ae6a771-9e99-4deb-ae5d-04bc3a9c020a"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dask as d\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from glob import glob\n",
    "from tensorflow import test, device\n",
    "# from tensorflow import keras \n",
    "from tensorflow.keras import backend, Input, Model, layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418507f9-8351-46c2-aae0-5dbabc7f3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fl_tissue_model_tools import defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56683c25-da47-41ea-8cbc-2984afc6b5e8",
   "metadata": {
    "id": "56683c25-da47-41ea-8cbc-2984afc6b5e8"
   },
   "outputs": [],
   "source": [
    "data_root_path = \"D:/oxford_pets_data\" # Carson\n",
    "# data_root_path = \"./\" # Mitchell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce606c-aa2d-49a6-b84f-7fc39a02e30c",
   "metadata": {
    "id": "54ce606c-aa2d-49a6-b84f-7fc39a02e30c"
   },
   "outputs": [],
   "source": [
    "img_paths = sorted([fn.replace(\"\\\\\", \"/\") for fn in glob(f\"{data_root_path}/images/*.jpg\")])\n",
    "label_paths = sorted([fn.replace(\"\\\\\", \"/\") for fn in glob(f\"{data_root_path}/annotations/trimaps/[!._]*.png\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e683fd5-39e2-4a5c-b10d-d132af48e4bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4a833-d384-439c-84ed-421555a8e83a",
   "metadata": {
    "id": "eeb4a833-d384-439c-84ed-421555a8e83a"
   },
   "outputs": [],
   "source": [
    "def map2bin(lab, fg_vals, bg_vals, fg=1, bg=0):\n",
    "    fg_mask = np.isin(lab, fg_vals)\n",
    "    bg_mask = np.isin(lab, bg_vals)\n",
    "    lab_c = lab.copy()\n",
    "    lab_c[fg_mask] = fg\n",
    "    lab_c[bg_mask] = bg\n",
    "    return lab_c\n",
    "\n",
    "\n",
    "def augment(img, rot, hflip, vflip, expand_dims=True):\n",
    "    og_shape = img.shape\n",
    "    hw = img.shape[:2]\n",
    "    # Horizontal flip\n",
    "    if hflip:\n",
    "        img = cv2.flip(img, 1)\n",
    "    # Vertical flip\n",
    "    if vflip:\n",
    "        img = cv2.flip(img, 0)\n",
    "    # Rotation\n",
    "    rot_mat = cv2.getRotationMatrix2D((hw[1] // 2, hw[0] // 2), rot, 1.0)\n",
    "    \n",
    "    if expand_dims:\n",
    "        img = np.expand_dims(cv2.warpAffine(img, rot_mat, hw), 2)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afcf19-8a01-4925-8e70-132e957ff8ab",
   "metadata": {
    "id": "b7afcf19-8a01-4925-8e70-132e957ff8ab"
   },
   "source": [
    "# Validate images match labels in order & count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da7153-bf9c-4cdd-b841-872b7b7d105b",
   "metadata": {
    "id": "32da7153-bf9c-4cdd-b841-872b7b7d105b"
   },
   "outputs": [],
   "source": [
    "def get_img_id(img_path):\n",
    "    return img_path.split(\"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2f892-37f7-4f5d-b5aa-868130e6015d",
   "metadata": {
    "id": "23c2f892-37f7-4f5d-b5aa-868130e6015d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(all([get_img_id(img_paths[i]) == get_img_id(label_paths[i]) for i in range(len(img_paths))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52d2ec-0e25-4265-83b3-d5741e78219f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f52d2ec-0e25-4265-83b3-d5741e78219f",
    "outputId": "4bc55652-40ac-4ddb-ba17-f2a9db714aa0"
   },
   "outputs": [],
   "source": [
    "assert(len(img_paths) == len(label_paths))\n",
    "print(len(img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdbfe1-1960-4f5d-8959-835debf0d104",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71c54a-2fdc-4ae4-806a-c6541996af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 12345\n",
    "batch_size = 32\n",
    "img_size = (128, 128)\n",
    "# img_size = (160, 160)\n",
    "# num_classes = 2\n",
    "n_outputs = 1\n",
    "rs = np.random.RandomState(seed=rand_seed)\n",
    "# For collapsing mask into binary range\n",
    "fg_vals = [1, 3]\n",
    "bg_vals = [2]\n",
    "cp_filepath = \"oxford_pets_segmentation_best_weights.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f0ebc-d560-4a34-af2b-ff42c4765f19",
   "metadata": {},
   "source": [
    "# Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a5b22-abfc-4dd2-8b27-979bab68cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_idx = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29416c-a467-43be-908c-f25b4bb4fffe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "5c29416c-a467-43be-908c-f25b4bb4fffe",
    "outputId": "5e16e478-3640-4af1-988b-3644759faffc"
   },
   "outputs": [],
   "source": [
    "# img = cv2.imread(img_paths[preview_idx], cv2.IMREAD_GRAYSCALE)\n",
    "img = np.expand_dims(np.array(load_img(img_paths[preview_idx], target_size=img_size, color_mode=\"grayscale\", interpolation=\"lanczos\")), 2)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d1928-3108-46f5-b732-ff34ecff9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.expand_dims(\n",
    "    np.array(load_img(label_paths[preview_idx], target_size=img_size, color_mode=\"grayscale\", interpolation=\"nearest\")), 2\n",
    ")\n",
    "mask = map2bin(mask, fg_vals, bg_vals)\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d446d4-80c9-4c9f-8ffe-c1d7f61be9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466087fc-c514-4865-aa51-7335f86d5c5f",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74792e",
   "metadata": {
    "id": "3c74792e"
   },
   "outputs": [],
   "source": [
    "class OxfordPetsSequence(Sequence):\n",
    "    \"\"\"Helper to iterate over the data\"\"\"\n",
    "    # TODO: edit to turn off augmentations for validation \n",
    "    \n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths, random_state, fg_vals, bg_vals, augmentation_function=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "        self.rs: np.random.RandomState = random_state\n",
    "        self.fg_vals = fg_vals\n",
    "        self.bg_vals = bg_vals\n",
    "        self.augmentation_function = augmentation_function\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns the batch (input, target) at index `idx`\"\"\"\n",
    "        # Image index, offset by batch\n",
    "        i = idx * self.batch_size \n",
    "        \n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "\n",
    "        # Load the input images and convert them to grayscale\n",
    "        def load_x():\n",
    "            x = np.zeros((len(batch_input_img_paths),) + self.img_size + (1,), dtype=np.float32)\n",
    "            for j, path in enumerate(batch_input_img_paths):\n",
    "                # Ensure best quality downsampling (interpolation methods overview: https://stackoverflow.com/a/44083113)\n",
    "                img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\", interpolation=\"lanczos\")\n",
    "                img = np.expand_dims(img, 2) # add a third dimension to the array\n",
    "                x[j] = img\n",
    "            return x\n",
    "\n",
    "        # load the target images and condense the number of labels in the segmentation mask\n",
    "        def load_y():\n",
    "            y = np.zeros((len(batch_target_img_paths),) + self.img_size + (1,), dtype=np.uint8)\n",
    "            for j, path in enumerate(batch_target_img_paths):\n",
    "                # Use interpolation=\"nearest\" to ensure mask is only valid bit values\n",
    "                img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\", interpolation=\"nearest\")\n",
    "                # add a third dimension to the array\n",
    "                img = np.expand_dims(img, 2)\n",
    "                # Collapse the mask from three labels to two labels\n",
    "                img = map2bin(img, self.fg_vals, self.bg_vals)\n",
    "                y[j] = img\n",
    "            return y\n",
    "\n",
    "        x, y = d.compute((d.delayed(load_x)(), d.delayed(load_y)()))[0]\n",
    "        \n",
    "        if self.augmentation_function != None:\n",
    "            m = len(x)\n",
    "            # Cannot parallelize (random state ensures reproducibility)\n",
    "            rots = self.rs.choice([0, 90, 180, 270], size=m)\n",
    "            hflips = self.rs.choice([True, False], size=m)\n",
    "            vflips = self.rs.choice([True, False], size=m)\n",
    "\n",
    "            def aug_imgs(imgs):\n",
    "                return np.array([self.augmentation_function(imgs[i], rots[i], hflips[i], vflips[i]) for i in range(m)])\n",
    "            \n",
    "            x, y = d.compute((d.delayed(aug_imgs)(x), d.delayed(aug_imgs)(y)))[0]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b619b2d-c8d5-4c42-a793-e7875fc1102f",
   "metadata": {},
   "source": [
    "# Data generator demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aQet1vlnZRa-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "aQet1vlnZRa-",
    "outputId": "9e3d2afb-7135-46cb-f26b-aac4fb98d3f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pets_demo = OxfordPetsSequence(batch_size, img_size, img_paths, label_paths, rs, fg_vals, bg_vals, augment)\n",
    "start = time.time()\n",
    "X, y = pets_demo[1]\n",
    "stop = time.time()\n",
    "print(stop - start)\n",
    "plt.imshow(X[1][:,:,0], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(y[1][:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b354058-bc68-45b1-bc70-7feb088febd3",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214fbbb",
   "metadata": {
    "id": "d214fbbb"
   },
   "outputs": [],
   "source": [
    "def get_oxford_pets_model(img_size, num_classes):\n",
    "    inputs = Input(shape=img_size + (1,))\n",
    "    \n",
    "    ### Downsampling the inputs ###\n",
    "    \n",
    "    # compute an initial set of 32 features using convolutional layers \n",
    "    # also, downsample the image using strided convolutions\n",
    "    x = layers.Conv2D(32, 3, strides = 2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    previous_block_activation = x\n",
    "    \n",
    "    # hidden layers using Xception convolutions\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "        \n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        \n",
    "        x = layers.add([x, residual])\n",
    "        \n",
    "        previous_block_activation = x\n",
    "        \n",
    "    ### upsampling ###\n",
    "    \n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x\n",
    "        \n",
    "    # add a per-pixel classification layer\n",
    "    # outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "    outputs = layers.Conv2D(n_outputs, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    \n",
    "    # define the model \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a50a3-03df-4d5e-a1df-22d12e8ae232",
   "metadata": {},
   "source": [
    "# Set up data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395ec50-6f31-42b1-8603-483dfb46dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = int(len(img_paths) * 0.2)\n",
    "n_test = int(len(img_paths) * 0.2)\n",
    "# Shuffle data\n",
    "data_idx = np.array(range(len(img_paths)))\n",
    "rs.shuffle(data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca49c3-d117-4396-9a0b-4c971abe0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_paths = img_paths[: -(n_val + n_test)]\n",
    "train_label_paths = label_paths[: -(n_val + n_test)]\n",
    "\n",
    "val_img_paths = img_paths[-(n_val + n_test): -n_test]\n",
    "val_label_paths = label_paths[-(n_val + n_test): -n_test]\n",
    "\n",
    "test_img_paths = img_paths[-n_test:]\n",
    "test_label_paths = label_paths[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed31941-d395-4ba9-b4e2-4251b96ac82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = OxfordPetsSequence(batch_size, img_size, train_img_paths, train_label_paths, rs, fg_vals, bg_vals, augment)\n",
    "val_gen = OxfordPetsSequence(batch_size, img_size, val_img_paths, val_label_paths, rs, fg_vals, bg_vals, augment)\n",
    "# No augmentation for test_gen\n",
    "test_gen = OxfordPetsSequence(batch_size, img_size, test_img_paths, test_label_paths, rs, fg_vals, bg_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106a809",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4106a809",
    "outputId": "1368003a-10b5-455b-ca8f-f0bb37612f99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### create the model ###\n",
    "backend.clear_session()\n",
    "# model = get_oxford_pets_model(img_size, n_classes)\n",
    "model = get_oxford_pets_model(img_size, n_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1cEkAMZL614",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1cEkAMZL614",
    "outputId": "9e20812a-4262-4826-f00a-341d2aeda0ca"
   },
   "outputs": [],
   "source": [
    "# tell tf to use the gpu\n",
    "# import tensorflow as tf\n",
    "device(test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61ecef-2708-4796-af8a-440a9ce5d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dab812",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62dab812",
    "outputId": "dfd01172-d20d-498c-8813-1797666dae7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### train the model ###\n",
    "# model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
    "callbacks = [ModelCheckpoint(cp_filepath, save_best_only=True, save_weights_only=True)]\n",
    "num_epochs = 50\n",
    "h = model.fit(train_gen, validation_data=val_gen, epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511ba11-a4bc-49c5-9681-28f174643cb8",
   "metadata": {},
   "source": [
    "# Load best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3183f-0519-4d5a-8054-befe786f71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(cp_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oTHI91F0tRXk",
   "metadata": {
    "id": "oTHI91F0tRXk"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all images in the test set\n",
    "# val_gen = OxfordPetsSequence(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
    "test_preds = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e346d-aede-4e67-98da-fac38078ecb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred = np.argmax(test_preds[pred_idx], axis=-1)\n",
    "pred_idx = 934\n",
    "pred = np.copy(test_preds[pred_idx])\n",
    "pred[pred < 0.5] = 0\n",
    "pred[pred > 0] = 1\n",
    "# true = test_img_paths[pred_idx]\n",
    "true = np.expand_dims(np.array(load_img(test_img_paths[pred_idx], target_size=img_size, color_mode=\"grayscale\", interpolation=\"lanczos\")), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a9146-3e09-42b5-8b3a-11375e851044",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(true, cmap=\"gray\", vmin=defs.GS_MIN, vmax=defs.GS_MAX)\n",
    "plt.show()\n",
    "plt.imshow(pred * defs.GS_MAX, cmap=\"gray\", vmin=defs.GS_MIN, vmax=defs.GS_MAX)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345d935",
   "metadata": {
    "id": "5345d935"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "oxford_pets_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
