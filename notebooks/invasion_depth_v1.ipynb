{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c12c68d-93a1-41af-b05c-e56b17582e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import dask as d\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc59e17-d035-4e8d-ad5f-ca7adfa21f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fl_tissue_model_tools import data_prep, dev_config, models, defs\n",
    "import fl_tissue_model_tools.preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c960e34-fb3c-463b-8bc5-952730a064dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = dev_config.get_dev_directories(\"../dev_paths.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f26842-80d0-4710-ab9f-3f19c1029199",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_path = f\"{dirs.data_dir}/invasion_data/development\"\n",
    "resnet_inp_shape = (128, 128, 3)\n",
    "# Binary classification -> only need 1 output unit\n",
    "n_outputs = 1\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "# n_epochs = 10\n",
    "frozen_epochs = 20\n",
    "fine_tune_epochs = 20\n",
    "fine_tune_lr = 1e-5\n",
    "# seed = 123\n",
    "seed = 20394\n",
    "class_labels = {\"no_invasion\": 0, \"invasion\": 1}\n",
    "\n",
    "# Early stopping\n",
    "es_criterion = \"val_loss\"\n",
    "es_mode = \"min\"\n",
    "# Update these depending on seriousness of experiment\n",
    "es_patience = 10\n",
    "es_min_delta = 0.0001\n",
    "\n",
    "# Model saving\n",
    "cp_criterion = \"val_loss\"\n",
    "cp_mode = \"min\"\n",
    "cp_frozen_filepath = \"../model_training/resnet50_invasion_depth_demo_v1_best_frozen_weights.h5\"\n",
    "cp_fine_tune_filepath = \"../model_training/resnet50_invasion_depth_demo_v1_best_fine_tune_weights.h5\"\n",
    "cp_best_only = True\n",
    "# Need to set to True otherwise base model \"layer\" won't save/load properly\n",
    "cp_weights_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da805bd0-d5c8-4dc4-a139-3a36eb018818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_class_weights_from_counts(d):\n",
    "    n = np.sum(list(d.values()))\n",
    "    n_c = len(d.keys())\n",
    "    weights = {}\n",
    "    for ci, n_ci in d.items():\n",
    "        weights[ci] = n / (n_c * n_ci)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e6e22-ffed-459e-8895-f1917396c661",
   "metadata": {},
   "source": [
    "# Prep for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d22b149-c675-4b1c-95e7-909716249d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a69dfe-afa0-41fa-8f26-c8463105bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_paths = {k: glob(f\"{root_data_path}/train/{k}/*.tif\") for k in class_labels.keys()}\n",
    "# img_counts = {c: len(pn) for c, pn in img_paths.items()}\n",
    "# class_weights = balanced_class_weights_from_counts(img_counts)\n",
    "data_paths = {v: glob(f\"{root_data_path}/train/{k}/*.tif\") for k, v in class_labels.items()}\n",
    "for k, v in data_paths.items():\n",
    "    rs.shuffle(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940352be-2f0d-4e8a-845b-8873c3629a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_counts = {k: len(v) for k, v in data_paths.items()}\n",
    "val_counts = {k: round(v * val_split) for k, v in data_counts.items()}\n",
    "train_counts = {k: v - val_counts[k] for k, v in data_counts.items()}\n",
    "train_class_weights = balanced_class_weights_from_counts(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9bdee1-cf3a-4713-800e-750180b6aa06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 390, 1: 39}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48279d3-14b3-43a7-80c4-fd99ddf41749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5496794871794872, 1: 5.532258064516129}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a3b38c5-6c48-4858-aec1-de470844072a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_paths = {k: v[val_counts[k]:] for k, v in data_paths.items()}\n",
    "val_data_paths = {k: v[:val_counts[k]] for k, v in data_paths.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105535b-e87d-4efb-9045-6cc547f17a23",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aff4bfe-ba64-4a4f-a316-6a7607cc9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvasionDataGenerator(Sequence):\n",
    "    def __init__(self, data_paths, class_labels, batch_size, img_shape, random_state, class_weights=None, shuffle=True, augmentation_function=None):\n",
    "        self.data_paths = deepcopy(data_paths)\n",
    "        self.batch_size = batch_size\n",
    "        self.img_shape = img_shape\n",
    "        self.class_labels = deepcopy(class_labels)\n",
    "        self.class_paths = {}\n",
    "        self.class_counts = {}\n",
    "        self.img_paths = []\n",
    "        self.img_labels = []\n",
    "        self.shuffle = shuffle\n",
    "        self.rs = random_state\n",
    "        self.augmentation_function = augmentation_function\n",
    "        self._get_paths_and_counts(data_paths)\n",
    "        self.indices = np.arange(len(self.img_paths), dtype=np.uint)\n",
    "        if class_weights != None:\n",
    "            self.class_weights = deepcopy(class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        self.shuffle_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len()\n",
    "        return len(self.img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        batch_idx_start = index * self.batch_size\n",
    "        batch_idx_end = batch_idx_start + batch_size\n",
    "        batch_indices = self.indices[batch_idx_start: batch_idx_end]\n",
    "\n",
    "        img_paths = [self.img_paths[i] for i in batch_indices]\n",
    "        # Should it be (B,) or (B,1)?\n",
    "        y = np.array([self.img_labels[i] for i in batch_indices])\n",
    "\n",
    "        # Generate data\n",
    "        # X = np.array([self.prep_image(ip) for ip in img_paths])\n",
    "        X = self.prep_images(img_paths)\n",
    "        \n",
    "        if self.augmentation_function != None:\n",
    "            X = self.augmentation_function(X, self.rs, expand_dims=False)\n",
    "        \n",
    "        if self.class_weights != None:\n",
    "            # Weight classes by relative proportions in the training set\n",
    "            w = np.array([self.class_weights[y_] for y_ in y])\n",
    "            return X, y, w\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def _get_paths_and_counts(self, data_paths):\n",
    "        # self.class_paths = {v: glob(f\"{self.datapath}/{k}/*.tif\") for k, v in self.class_labels.items()}\n",
    "        self.class_paths = deepcopy(data_paths)\n",
    "        self.class_counts = {c: len(pn) for c, pn in self.class_paths.items()}\n",
    "        for k, v in self.class_paths.items():\n",
    "            # Paths to each image\n",
    "            self.img_paths.extend(v)\n",
    "            # Associate labels with each image path\n",
    "            self.img_labels.extend(list(np.repeat(k, len(v))))\n",
    "            \n",
    "    def _load_img(self, path):\n",
    "        img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
    "        img = prep.min_max_(cv2.resize(img, self.img_shape, cv2.INTER_LANCZOS4).astype(np.float32), defs.GS_MIN, defs.GS_MAX, defs.TIF_MIN, defs.TIF_MAX)\n",
    "        img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n",
    "        return img\n",
    "            \n",
    "    def shuffle_indices(self):\n",
    "        # print(\"shuffling\")\n",
    "        self.rs.shuffle(self.indices)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.img_paths), dtype=np.uint)\n",
    "        if self.shuffle == True:\n",
    "            self.shuffle_indices()\n",
    "\n",
    "    def prep_images(self, paths):\n",
    "        # imgs = np.zeros((self.batch_size,) + self.img_shape + (3,))\n",
    "        \n",
    "        # for i, path in enumerate(paths):\n",
    "        #     img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
    "        #     # img = prep.min_max_(cv2.resize(img, self.img_shape, cv2.INTER_LANCZOS4).astype(np.float32), 0, 1, defs.TIF_MIN, defs.TIF_MAX)\n",
    "        #     img = prep.min_max_(cv2.resize(img, self.img_shape, cv2.INTER_LANCZOS4).astype(np.float32), defs.GS_MIN, defs.GS_MAX, defs.TIF_MIN, defs.TIF_MAX)\n",
    "        #     img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n",
    "        #     imgs[i] = img\n",
    "        imgs = np.array(d.compute((d.delayed(self._load_img)(p) for p in paths))[0])\n",
    "        return resnet50.preprocess_input(imgs)\n",
    "        # return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d89ed15-f0c8-4469-946c-f0c0e6cbb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With class weights\n",
    "train_datagen = InvasionDataGenerator(train_data_paths, class_labels, batch_size, resnet_inp_shape[:2], rs, class_weights=train_class_weights, augmentation_function=prep.augment_imgs)\n",
    "val_datagen = InvasionDataGenerator(val_data_paths, class_labels, batch_size, resnet_inp_shape[:2], rs, class_weights=train_class_weights, augmentation_function=prep.augment_imgs)\n",
    "# Without class weights\n",
    "# train_datagen = InvasionDataGenerator(train_data_paths, class_labels, batch_size, resnet_inp_shape[:2], rs, augmentation_function=prep.augment_imgs)\n",
    "# val_datagen = InvasionDataGenerator(val_data_paths, class_labels, batch_size, resnet_inp_shape[:2], rs, augmentation_function=prep.augment_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e81cb-bfd6-4ac8-a36d-da6ca500c5f3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f59ef1d-0f95-4a14-a92d-327229a104a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5bbe2fe-8e5b-4e69-a87d-81dcee513b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model = models.build_ResNet50_TL(\n",
    "    n_outputs,\n",
    "    resnet_inp_shape,\n",
    "    base_last_layer=\"conv5_block3_out\",\n",
    "    # base_last_layer=\"conv5_block2_out\",\n",
    "    # base_last_layer=\"conv5_block1_out\",\n",
    "    # base_last_layer=\"conv4_block6_out\",\n",
    "    # base_last_layer=\"conv4_block3_out\",\n",
    "    # base_last_layer=\"conv4_block1_out\",\n",
    "    # base_last_layer=\"conv3_block4_out\",\n",
    "    # Switch to softmax once n_outputs > 1\n",
    "    output_act=\"sigmoid\",\n",
    "    base_model_trainable=False\n",
    ")\n",
    "tl_model.compile(optimizer=Adam(), loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "134be685-feda-4ef2-b1f3-e431dcf88233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "base_model (Functional)      (None, 4, 4, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 2,049\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tl_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a30cd0a-6cef-4e3b-b85a-b95be2cd08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(monitor=es_criterion, mode=es_mode, min_delta=es_min_delta, patience=es_patience)\n",
    "cp_callback = ModelCheckpoint(cp_frozen_filepath, monitor=cp_criterion, mode=cp_mode, save_best_only=cp_best_only, save_weights_only=cp_weights_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c38e7b3e-8344-49f4-8c52-70d9ae017acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 10s 318ms/step - loss: 0.9425 - binary_accuracy: 0.3406 - val_loss: 0.7226 - val_binary_accuracy: 0.5156\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 2s 186ms/step - loss: 0.5829 - binary_accuracy: 0.8438 - val_loss: 0.9932 - val_binary_accuracy: 0.8594\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.5847 - binary_accuracy: 0.6969 - val_loss: 0.6064 - val_binary_accuracy: 0.5781\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.5391 - binary_accuracy: 0.7656 - val_loss: 0.8574 - val_binary_accuracy: 0.8125\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 2s 189ms/step - loss: 0.5202 - binary_accuracy: 0.7188 - val_loss: 0.6094 - val_binary_accuracy: 0.5938\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.4004 - binary_accuracy: 0.8031 - val_loss: 0.7474 - val_binary_accuracy: 0.8438\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 2s 189ms/step - loss: 0.3896 - binary_accuracy: 0.8313 - val_loss: 0.6305 - val_binary_accuracy: 0.7031\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 2s 188ms/step - loss: 0.3659 - binary_accuracy: 0.7719 - val_loss: 0.3273 - val_binary_accuracy: 0.8906\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 2s 187ms/step - loss: 0.3593 - binary_accuracy: 0.9031 - val_loss: 0.2626 - val_binary_accuracy: 0.8750\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 2s 190ms/step - loss: 0.3673 - binary_accuracy: 0.7656 - val_loss: 0.5047 - val_binary_accuracy: 0.6719\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 2s 192ms/step - loss: 0.3154 - binary_accuracy: 0.8719 - val_loss: 0.5090 - val_binary_accuracy: 0.8750\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 2s 192ms/step - loss: 0.3059 - binary_accuracy: 0.9094 - val_loss: 0.5030 - val_binary_accuracy: 0.8594\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 2s 202ms/step - loss: 0.3488 - binary_accuracy: 0.7531 - val_loss: 0.4749 - val_binary_accuracy: 0.8125\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 0.3199 - binary_accuracy: 0.8625 - val_loss: 0.4807 - val_binary_accuracy: 0.8906\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.3120 - binary_accuracy: 0.8844 - val_loss: 0.5024 - val_binary_accuracy: 0.7500\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.2587 - binary_accuracy: 0.8469 - val_loss: 0.6307 - val_binary_accuracy: 0.8750\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.3215 - binary_accuracy: 0.9250 - val_loss: 0.4617 - val_binary_accuracy: 0.7812\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.2696 - binary_accuracy: 0.8438 - val_loss: 0.4179 - val_binary_accuracy: 0.7812\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.2592 - binary_accuracy: 0.8938 - val_loss: 0.5141 - val_binary_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "h1 = tl_model.fit(\n",
    "    train_datagen,\n",
    "    validation_data=val_datagen,\n",
    "    epochs=frozen_epochs,\n",
    "    callbacks=[es_callback, cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21a2e1-6fb0-4252-97b6-7f395a21aa98",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load best frozen weights before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f727e052-d9c1-4613-8582-d93c31e46047",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model.load_weights(cp_frozen_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04b9f7-0220-47e1-88a1-b375166403e7",
   "metadata": {},
   "source": [
    "# Train model (all layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "826188ac-9b5f-4e90-8d9f-2fcaeaa1a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make base model trainable (leave layers in inference mode)\n",
    "models.toggle_TL_freeze(tl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fce61bd-18b7-43e0-96f9-a309c763888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model.compile(optimizer=Adam(learning_rate=fine_tune_lr), loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff63ccae-1830-4cda-ad44-a8bc1436562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "base_model (Functional)      (None, 4, 4, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tl_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bc0da73-0193-4e18-bc4c-f84d86e17a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(monitor=es_criterion, mode=es_mode, min_delta=es_min_delta, patience=es_patience)\n",
    "cp_callback = ModelCheckpoint(cp_fine_tune_filepath, monitor=cp_criterion, mode=cp_mode, save_best_only=cp_best_only, save_weights_only=cp_weights_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c2c008-c5fe-473f-a209-9da1e56b64b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 11s 340ms/step - loss: 0.4761 - binary_accuracy: 0.7312 - val_loss: 0.4041 - val_binary_accuracy: 0.7656\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.3517 - binary_accuracy: 0.9156 - val_loss: 0.3770 - val_binary_accuracy: 0.8281\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 2s 199ms/step - loss: 0.2187 - binary_accuracy: 0.8250 - val_loss: 0.9807 - val_binary_accuracy: 0.9062\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.2834 - binary_accuracy: 0.8844 - val_loss: 0.4988 - val_binary_accuracy: 0.7656\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 2s 203ms/step - loss: 0.2388 - binary_accuracy: 0.9031 - val_loss: 0.3611 - val_binary_accuracy: 0.8750\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.1899 - binary_accuracy: 0.9187 - val_loss: 0.3205 - val_binary_accuracy: 0.9062\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 2s 189ms/step - loss: 0.1777 - binary_accuracy: 0.9312 - val_loss: 0.4788 - val_binary_accuracy: 0.8906\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 2s 191ms/step - loss: 0.1244 - binary_accuracy: 0.9375 - val_loss: 0.7845 - val_binary_accuracy: 0.9062\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 2s 189ms/step - loss: 0.1422 - binary_accuracy: 0.9375 - val_loss: 0.2771 - val_binary_accuracy: 0.8906\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 2s 190ms/step - loss: 0.1463 - binary_accuracy: 0.9187 - val_loss: 0.3238 - val_binary_accuracy: 0.9531\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 2s 191ms/step - loss: 0.1151 - binary_accuracy: 0.9250 - val_loss: 0.2658 - val_binary_accuracy: 0.9219\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 2s 188ms/step - loss: 0.1008 - binary_accuracy: 0.9688 - val_loss: 0.2019 - val_binary_accuracy: 0.9062\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.0783 - binary_accuracy: 0.9563 - val_loss: 0.6327 - val_binary_accuracy: 0.9531\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 2s 188ms/step - loss: 0.2153 - binary_accuracy: 0.9250 - val_loss: 0.4150 - val_binary_accuracy: 0.7500\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 2s 191ms/step - loss: 0.1587 - binary_accuracy: 0.9375 - val_loss: 0.4574 - val_binary_accuracy: 0.9375\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 2s 193ms/step - loss: 0.1217 - binary_accuracy: 0.9375 - val_loss: 0.4149 - val_binary_accuracy: 0.8906\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 2s 192ms/step - loss: 0.0823 - binary_accuracy: 0.9719 - val_loss: 0.3409 - val_binary_accuracy: 0.9219\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 2s 202ms/step - loss: 0.0651 - binary_accuracy: 0.9656 - val_loss: 0.3315 - val_binary_accuracy: 0.9062\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 2s 224ms/step - loss: 0.0570 - binary_accuracy: 0.9875 - val_loss: 0.4314 - val_binary_accuracy: 0.9531\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.0595 - binary_accuracy: 0.9625 - val_loss: 0.3831 - val_binary_accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "h2 = tl_model.fit(\n",
    "    train_datagen,\n",
    "    validation_data=val_datagen,\n",
    "    epochs=fine_tune_epochs,\n",
    "    callbacks=[es_callback, cp_callback]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
